### 우리FISA 9주차 학습기록
#### ( 2025.03.04 ~ 2025.03.07 )
***
##### 2025.03.04. Tue
MLops

#### K-Means Algorithm

**실루엣 계수**
각 클러스터의 응집도가 얼마나 되는지
군집 내 거리는 짧을수록 다른 군집과의 거리는 멀 수록 좋다!

1에 가까울 수록 좋은 군집이라고 평가


i. 원본의 결과(사전에 정의된 그룹)를 모를 때
원본과 비교시 잘 뭉쳐있는지 : 균질성
각 실제 값에 대해 동일한 군집으로 구성된 정도 : 완전성

ii. 원본의 결과를 알 때


각 군집의 평균 응집도 : inertia_

우리가 가지고 있는 데이터가 다 연속형 변수일 때
다음달 카드 값이 얼마나 나올지 예측

6개월 평균 소비액/최근 3개월 평균
1. 두 변수의 상관관계를 본다. (너무 상관관계가 크면 지워줌)


#### DBSCAN
밀도 기반 **클러스터링 알고리즘**
특정 밀도 이상으로 몰려 있는 데이터를 클러스터로 그룹화하고, 밀도가 낮은 영역의 포인트는 노이즈로 분류하는 방법

eps: 이웃으로 판단하는 최대거리
min_samples: 이웃 반경내 최소 샘플수

장점 : K의 개수를 정하지 않아도 된다., 노이즈(이상치)도 알아서 분류해줌
단점 : 데이터가 많으면 많은 시간들이 걸리게 된다.

PCA VS DBSCAN

#### 클러스터링 기준
콘텐츠 기반 클러스터링


유저 기반 클러스터링


협업 기반 클러스터링
유저의 개인적인 선호도 + 다른 사람들의 일반적인 선호도

문제)콜드 스타트(Cold Start)
새로 시작할 때 곤란함 -> 데이터가 없어서 뭘 보여줘야될 지 모르겠어ㅠㅠ

클러스터링의 결과를 데이터를 EDA 하는 과정에서 파생변수로 사용할 수도 있다.

TREE
반반이 되는 특성으로 해서 순도가 1이 될 때까지 나눈다.

#### AUTO ML
**Isolation forest**
트리 알고리즘이기 때문에
전처리 할 때도 많이 사용 -> 이상치를 의사결정나무 상단부에서 분리 가능

#### SMOTE(Synthetic Minority Over-sampling Technique)
불균형한 데이터셋에서 소수 클래스(minority class)의 데이터를 인공적으로 생성하여 균형을 맞추는 오버샘플링 기법

K라는 파라미터를 정해주면 거리 평균을 가지고 오버샘플링 해줌



실습
train_df와 val_df 특성를 비교

재현율을 높이면 정밀도가 떨어지는 문제점



- 실습 중..
뭐가 이상치고 정상인지 학습 과정에서 넘겨주지 않았다.
실제로 어느정도 성능을 가지고 있는지 확이해야되기 대문에 성능평가만 분류의 방법으로 사용하고 있는 중인 것이다.





***
#### 우리FISA 40일차 KPT
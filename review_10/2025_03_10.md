### 우리FISA 10주차 학습기록
#### ( 2025.03.10 ~ 2025.03.14 )
***
##### 2025.03.10. Mon

#### CNN(Convolutional Neural Network)
이미지 처리에 특화된 딥러닝 모델

**🔑 핵심 연산 - 컨볼루션 연산**
- 입력 데이터(예: 이미지)와 작은 필터(커널)를 곱하고 더해 특징을 추출하는 과정
- 연산이 완료된 결과 데이터를 특징 맵(feature map)이라고 불렀다.
- 필터의 사이즈는 홀수(짝수면 패딩이 비대칭), 과적합을 방지하는 역할

<br>

##### CNN에서 조절할 수 있는 하이퍼파라미터
1. Padding: 입력 데이터의 크기를 유지하거나 증가시키기 위해 가장자리에 추가하는 값(주로 0)이다.
2. Stride: 필터가 입력 데이터를 따라 이동하는 간격으로, 값이 클수록 출력 크기가 작아진다.

##### 풀링(Pooling)
특성 맵의 크기를 줄여 계산량을 감소시키고 중요한 정보만 유지하는 과정
- 최대 풀링(Max Pooling) : 가장 큰 값 선택
- 평균 풀링(Avg Pooling) : 평균 값을 선택

➡️ 한마디로 큰 사진을 작은 사진으로 줄이는 것! 필요한 정보만 남아 빠른 계산이 가능하다.


#### 과적합(Overfitting)
모델이 훈련 데이터에 너무 잘 맞아버려서 새로운 데이터(테스트 데이터)에서는 성능이 떨어지는 현상
➡️ 최대한 과적합이 오는 시기를 늦추는 것이 중요

<br>

##### 과적합을 방지하는 방법들
CNN 레이어
**Batch Normalization (배치 정규화)**
학습 과정에서 각 층의 입력을 정규화해서 학습을 안정적으로 만들고 과적합을 줄여줘. -> 토닥토닥 평균과 분산을 0또는 1로 만들어준다
감마와 시그마

**Dropout**
학습할 때 일부 뉴런을 랜덤하게 꺼버려서 특정 뉴런에 너무 의존하지 않도록 만든다.

➡️ Dropout과 Batch Normalization은 같이 사용하면 효율이 떨어진다.

Scaling 후 효과 없다.. -> BN 실행

**Max Pooling (최대 풀링)**
특성 맵의 크기를 줄이면서 중요한 특징만 남겨서 불필요한 정보가 과적합을 일으키지 않도록 도와줘.


**데이터 증강(Data Augmentation)**
훈련 데이터를 회전, 뒤집기, 밝기 조절 등으로 다양하게 변형해서 데이터 수를 늘리고, 과적합을 줄여.


L1, L2 정규화 (Lasso, Ridge Regularization)
가중치가 너무 커지지 않도록 벌점을 줘서 모델이 너무 복잡해지는 걸 방지해.


**Early Stopping (조기 종료)**
검증 데이터의 성능이 더 이상 좋아지지 않으면 학습을 멈춰서 과적합이 시작되기 전에 종료하는 방법이야.
더 많은 데이터 사용

데이터가 많을수록 모델이 훈련 데이터에만 지나치게 맞춰지는 과적합을 줄일 수 있어.



#### 전이학습과 YOLO

gamma와 beta는 훈련되는 파라메터가 아님, 임시저장 파라미터임

#### 전이학습
한 문제에서 학습한 모델의 지식을 다른 관련 문제에 활용하는 기법
예시) 사람의 몸에 대해서 학습한 모델은 사람이 입은 옷에 대해서도 잘 학습할 것이다!

장점 :  빠른 학습속도, 적은 데이터셋으로도 좋은 성능, 일반화 성능 향상, 계산 자원 및 시간 절약
단점 : 학습데이터와 도메인이 다를 경우 반대방향으로 학습할 수 있음
fine tunning 과정이 까다로울 수 있다.
기존 모델이 학습한 편향이나 오류가 새로운 작업에도 그대로 영향을 미칠 수 있다.


#### YOLO (You Only Look Once)
이미지 탐지 모델은 영역이 더 큰 걸을 인식한다.
이미지 전체를 한꺼번에 본다.

#### NMS의 과정
전체중에 얼만큼을 포함하는 가 -> precision과 recall의 문제

Tensor Board에서 실시간으로 데이터가 어떻게 학습되는지 볼 수 있다.
➡️ 추가학습을 할지말지, 하이퍼파라미터를 변경할지 말지 정할 수 잇다.


***
#### 우리FISA 44일차 KPT
Keep : 수업 집중도 굿~

Problem : 기억하려고 애쓰기

Try : 30분만 투자해서 복습하기! (다 못했다 하더라도 이 이상 하지 말기, 이거 말고도 할 거 많다.)
### 우리FISA 10주차 학습기록
#### ( 2025.03.10 ~ 2025.03.14 )
***
##### 2025.03.10. Fri
빅데이터 시스템을 다루기 위한 우분투, 도커, 엘라스틱서치

#### CNN(Convolutional Neural Network)
이미지 처리에 특화된 딥러닝 모델

**🔑 핵심 연산 - 컨볼루션 연산**
- 입력 데이터(예: 이미지)와 작은 필터(커널)를 곱하고 더해 특징을 추출하는 과정
- 연산이 완료된 결과 데이터를 특징 맵(feature map)이라고 불렀다.
- 필터의 사이즈는 홀수(짝수면 패딩이 비대칭), 과적합을 방지하는 역할

<br>

##### CNN에서 조절할 수 있는 하이퍼파라미터
1. Padding: 입력 데이터의 크기를 유지하거나 증가시키기 위해 가장자리에 추가하는 값(주로 0)이다.
2. Stride: 필터가 입력 데이터를 따라 이동하는 간격으로, 값이 클수록 출력 크기가 작아진다.

##### 풀링(Pooling)
특성 맵의 크기를 줄여 계산량을 감소시키고 중요한 정보만 유지하는 과정
- 최대 풀링(Max Pooling) : 가장 큰 값 선택
- 평균 풀링(Avg Pooling) : 평균 값을 선택

➡️ 한마디로 큰 사진을 작은 사진으로 줄이는 것! 필요한 정보만 남아 빠른 계산이 가능하다.


과적합(Overfitting)이란, 모델이 훈련 데이터에 너무 잘 맞아버려서 새로운 데이터(테스트 데이터)에서는 성능이 떨어지는 현상
➡️ 최대한 과적합이 오는 시기를 늦추는 것이 중요

<br>

##### 과적합을 방지하는 방법들
Batch Normalization (배치 정규화)

학습 과정에서 각 층의 입력을 정규화해서 학습을 안정적으로 만들고 과적합을 줄여줘.
Dropout

학습할 때 일부 뉴런을 랜덤하게 꺼버려서 특정 뉴런에 너무 의존하지 않도록 만들어.
Max Pooling (최대 풀링)

특성 맵의 크기를 줄이면서 중요한 특징만 남겨서 불필요한 정보가 과적합을 일으키지 않도록 도와줘.
데이터 증강(Data Augmentation)

훈련 데이터를 회전, 뒤집기, 밝기 조절 등으로 다양하게 변형해서 데이터 수를 늘리고, 과적합을 줄여.
L1, L2 정규화 (Lasso, Ridge Regularization)

가중치가 너무 커지지 않도록 벌점을 줘서 모델이 너무 복잡해지는 걸 방지해.
Early Stopping (조기 종료)

검증 데이터의 성능이 더 이상 좋아지지 않으면 학습을 멈춰서 과적합이 시작되기 전에 종료하는 방법이야.
더 많은 데이터 사용

데이터가 많을수록 모델이 훈련 데이터에만 지나치게 맞춰지는 과적합을 줄일 수 있어.

#### 전이학습과 YOLO

gamma와 beta는 훈련되는 파라메터가 아님, 임시저장 파라미터임

전이학습

YOLO (You Only Look Once)
이미지 탐지 모델은 영역이 더 큰 걸을 인식한다.
이미지 전체를 한꺼번에 본다.

NMS의 과정




***
#### 우리FISA 44일차 KPT
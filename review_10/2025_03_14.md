### 우리FISA 10주차 학습기록
#### ( 2025.03.10 ~ 2025.03.14 )
***
##### 2025.03.14. Fri
이번 달 교과목 평가 범위 : 빅데이터 시스템을 다루기 위한 우분투, 도커, 엘라스틱서치

#### LLM (Large Language Model, 대형 언어 모델)
자연어 처리(NLP) 모델

**Transformer 기반**
LLM은 Transformer 아키텍처를 기반
RNN/LSTM보다 병렬 연산이 가능하여 대규모 데이터 학습 가능
Self-Attention 메커니즘을 활용

**사전 훈련(Pre-training)과 미세 조정(Fine-tuning)**
- Pre-training: 인터넷의 방대한 텍스트 데이터로 초기 학습
- Fine-tuning: 특정 태스크(예: 감성 분석, 번역)에 맞춰 추가 학습


#### Attention
문장에서 중요한 부분을 더 집중해서 보는 기술
중요한 단어에 가중치를 더 줌
Attention은 번역할 때 각 단어가 어떤 단어와 연결되는지 가중치를 계산해 준다.

1. Query(질문), Key(열쇠), Value(값) 생성
2. 가중치 계산 (중요한 단어 찾기!)
3. 결과 벡터 생성

**Self-Attention vs Multi-Head Attention**
- Self-Attention: 한 문장 안에서 단어들끼리 서로 영향을 주는 것 (예: 문장 이해, 번역)
- Multi-Head Attention: 여러 개의 Attention을 병렬로 적용해 더 풍부한 정보를 학습 (예: Transformer에서 사용)

#### hugging face 사용방법
자연어 처리(NLP) 및 머신러닝(ML) 모델을 쉽게 활용할 수 있도록 지원하는 플랫폼
중간에 뜨는 메세지를 잘 확인해주어야한다.


***
#### 우리FISA 46일차 KPT
Keep : 없다.
Problem : 내가 해야될 일에 집중하기
Try : 수업시간에는 슬랙 알람 꺼놓기
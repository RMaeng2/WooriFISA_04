### 우리FISA 7주차 학습기록
#### ( 2025.02.17 ~ 2025.02.21 )
***
##### 2025.02.19. Wed
모델 성능 개선, 전처리


퍼셉트론의 목표
입력정보와 출력정보의 관계를 잘 찾도록 가중치 w를 조절하기 

어떻게 알 수 있을까?

경사하강법 
틀린게 0이 되도록 
0에서 얼마나 떨어져 있는 만큼 틀렸는가

지난번 학습에 얼만큼 반영 조금조금씩 내려가면서 최적화 지점을 찾게 됨

러닝레이트 = 스텝사이즈 = 학습률

너무 적게 학습률을 주면

1. 퍼셉트론을 사이킷런으로 바꿔보기
2. 로지스틱 회귀 해보기


선형으로 전달하기는 어려워요~

분류모델의 정확도를 높이기 위한 여러가지 방법론

결정 경계를 더 매끄럽게 만든다!


퍼셉트론을 했을 때 결정 경계가 매끄럽지 못한 경우가 있다.
-> 틀린 값을 선형으로 결과를 넘겨주어 가중치와 바이어스를 업데이트하도록 액티베이션 함수를 업데이트 했다.
넘겨준 특성을 바탕으로 결과를 예측할 때 오차를 수치로 넘겨주었다.
한 에폭마다 얼마나 오차가 쌓였는지 = 비용
가중치를 체크하게 만드는 손실함수 

목적함수가 가중치에 대해서 업데이트하는 과정에서 얼마나 반영할 것인지 정함 = 학습률


아달린

액티베이션 함수로 얼만큼 오차가 있었는지 선형으로 리턴

비용에 대해서 음이든 양이든 다 제곱해서 합을 한다.

모델이 최적의 가중치와 바이어스를 학습하는 방법에 대해서 


#### C 규제
C 규제를 이용해서 과대적합을 막는다.
절댓값으로 합을 하기 때문에 편향을 다 합산한다고 할 때 가중치의 합이 변하는 폭이 제곱일 때보다 한정되어있다.


l1, l2 정규화

***
#### 우리FISA 33일차 KPT
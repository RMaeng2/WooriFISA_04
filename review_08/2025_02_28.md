### 우리FISA 8주차 학습기록
#### ( 2025.02.24 ~ 2025.02.28 )
***
##### 2025.02.28. Fri

boosting model 중 많이 사용하는 모델

### 차원 축소를 사용한 데이터 압축

#### 차원의 저주
차원이 증가하면(특성의 개수가 증가할수록) 데이터 분석과 머신러닝 모델이 어려워진다.

차원이 많아지면 데이터 간의 밀도가 떨어짐
➡️ 개별 차원 내 학습할 데이터 수가 적어진다.(희소 현상)

#### PCA
데이터 중에 의미있는 데이터들만 추려내는데 사용
① 특성추출
② 차원 축소


1. scaling을 사용하면 pca를 하면 더 데이터의 분포를 잘 캡처할 수 있나?
스케일링 한다고 성능이 더 좋아지는 건 아니다.

2. 축을 늘리면 실제로 데이터의 분포를 잘 가져오면서 손실이 덜 일어나나?
차원이 크지 않으면 있는 그대로 넣는게 더 결과가 좋을 수 있다.

PCA는 데이터를 선형으로 계산할 수 있는 축을 N개 만든다.
비선형으로 패턴을 가진 데이터를 분류하기에 애매한 지점들이 생겨난다.
귀납기법을 사용해서 데이터들을 더 높은 차원으로 보내서 직선을 찾는다.
그리고 다시 낮은 차원으로 바꿔서 차원을 축소한다.
계산 속도는 약간 걸리지만 비선형 관계를 찾아서 분류 회귀를 할때 큰 효과를 발휘한다.

![alt text](/WooriFISA_04/review_08/img_8/image.png)

맹글링..?

***
#### 우리FISA 39일차 KPT
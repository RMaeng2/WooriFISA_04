### 우리FISA 8주차 학습기록
#### ( 2025.02.24 ~ 2025.02.28 )
***
##### 2025.02.28. Fri

boosting model 중 많이 사용하는 모델

### 차원 축소를 사용한 데이터 압축

#### 차원의 저주
차원이 증가하면(특성의 개수가 증가할수록) 데이터 분석과 머신러닝 모델이 어려워진다.

차원이 많아지면 데이터 간의 밀도가 떨어짐
➡️ 개별 차원 내 학습할 데이터 수가 적어진다.(희소 현상)

#### PCA
데이터 중에 의미있는 데이터들만 추려내는데 사용
① 특성추출
② 차원 축소

decomposition 라이브러리 안에 들어있는 함수이다.
비지도 학습이므로 X값만 넘겨주면 된다.

데이터를 모델에게 넘겨주기 전에 차원을 줄여나가기 위한 비지도학습

pca.explained_variance_ratio_



1. scaling을 사용하면 pca를 하면 더 데이터의 분포를 잘 캡처할 수 있나?
스케일링 한다고 성능이 더 좋아지는 건 아니다.

2. 축을 늘리면 실제로 데이터의 분포를 잘 가져오면서 손실이 덜 일어나나?
차원이 크지 않으면 있는 그대로 넣는게 더 결과가 좋을 수 있다.

PCA는 데이터를 선형으로 계산할 수 있는 축을 N개 만든다.
비선형으로 패턴을 가진 데이터를 분류하기에 애매한 지점들이 생겨난다.
귀납기법을 사용해서 데이터들을 더 높은 차원으로 보내서 직선을 찾는다.
그리고 다시 낮은 차원으로 바꿔서 차원을 축소한다.
계산 속도는 약간 걸리지만 비선형 관계를 찾아서 분류 회귀를 할때 큰 효과를 발휘한다.

데이터의 손실을 감수하면서 희소현상을 극복할 수 있는 좋은 축인가?!
![alt text](/WooriFISA_04/review_08/img_8/image.png)

맹글링..?

여러개의 특성에 대해서 모델이 학습하려면 차원들이 커질수록 많은 데이터가 필요한 차원의 저주

특성을 가장 많이 담을 수 있는 축을 정함

#### PCA에서 Elbow point
차원 축소 시 최적의 주성분의 개수를 결정하는데 사용한다.
데이터의 특징을 나타낼 수 있는 특징ㅇ

모델 성능 개선
파이프라인의 객체에다 standard scaling


***
#### 우리FISA 39일차 KPT
### 우리FISA 8주차 학습기록
#### ( 2025.02.24 ~ 2025.02.28 )
***
##### 2025.02.27. Thu

### 지도학습 - 회귀(Linear Regression)

예측값이 연속형일 때 사용하기 적절하다.
➡️ 일반 회귀의 목표는 X와 Y사이의 선형 관계를 찾는 것
➡️ 범주형은 직선으로 표현이 어렵다.

선형 관계 : X가 변할 때 Y가 일정한 비율로 변한다. → 직선 그래프
비선형 관계 : X가 변할 때 Y도 변하지만 일정 비율이 아니다. → 곡선 그래프

종속변수(X)를 보고 독립변수(Y)을 수식으로 표현해서 예측할 수 있다.

![alt text](/WooriFISA_04/review_08/img_8/image-1.png)

X와 Y 사이의 공식
Y = aX+b

|변수|의미|
|--|--|
|X|예측에서 사용할 값|
|Y|결과값|
|a|기울기(변화량, 얼마나 영향을 주는지)|
|b|시작점(기본 값)|

<br>

**❓. 종속변수가 여러 개여도 직선으로 표현이 가능한가?**

단순 회귀 : 종속변수(X)가 1개
다중 회귀 : 종속변수(X)가 2개 이상

|X의 개수||
|--|--|
|1개|2D 좌표 평면에서 직선|
|2개|3D 공간에서의 평면|
|3개 이상|고차원 공간에서의 초평면|

회귀선 : 데이터(X, Y)의 관계를 가장 잘 설명하는 **최적의 예측 라인**
경계보다 크면 1, 경계보다 작으면 0 반환


### 회귀모델의 평가지표



음의 값, 양의 값 가리지 않고 다 더해서 평균적으로 얼마나 떨어져 있는지 측정했는데요

0에 가까울수록 좋은 값, 큰 값은 1을 넘어서 얼마든지 커질수 있다.

MSE랑 MAE가 뭐가 다를까요?

intercepy_: 상수항의 역할


오차행렬 
연속형 변수에서는 _, _로는 평가할 수 없다.

오차를 쌓아 나가는 개념이기 때문에 어떤 지표를 선택하건 보조적으로 보게되는 결정계수(R2 score)
실제 값의 평균에서 각각의 점들을 뺀다면 합이 0이 될 것이다.
1에 가까울수록 좋은 모델 0으로 가까울수록 나쁜 모델

오차를 제곱하느냐 (MSE)
멀리 떨어져 있는 애한테 가중치를 더 크게 준다.


오차를 절댓값으로 계산하냐 (MAE)
음양 상관없이 계산해서 단위 자체를 가져올 수 있다. 오차 크기 그대로 반영한다.

오차 그 자체만을 가지고 계산을 하기 때문에 

편차가 있어도 똑같은 값이 나오게 된다.


하나의 특성으로 하나의 결과를 설명할때보다
여러 특성으로 하나의 결과를 설명할 때 오차가 줄어든다.

하이퍼파라미터를 설정하지 않은 베이스라인을 먼저 만들어놓는다

joblib?
python에 바이너리 패키지로 만든다!
load를 이용해 그대로 쓸 수 있다.

스케일링한다고 결과가 다 좋아지는 건 아니다.
상관관계가 큰 특성들을 추려서 학습하면 성능이 더 좋아질 것으로 생각하는데 아니다.
머신러닝 모델에서는 0.1만 넘어도 음으로 양으로 상관관계가 있다고 여겨진다.

전체 특성들 중에서 하나씩 값을 빼보면서 어떤 특성이 어느정도 영향을 미치는지 파악하고 0과 1로 구분하는 SequentialFeatureSelector


각각의 데이터 포인트들에 대해서 과대적합될 수도 있으므로 가중치를 학습하는과정에서 가중치들의 합을 제한하거나 규제를 사용

<Boston 집 값 예측>
음의 상관관계/ 양의 상관관계


Ridge, Lasso 모델

절대값으로 가중치들을 합산해서 계산하기 때문에 실제로 영향력 있는 가중치들을 만났을 때 더 크게 학습

하나의 모델을 쓰는 것보다 n개의 모델의 다수결을 출력하게 되면 새로운 데이터에 대한 성능이 높아진다.

Extra Tress Regressor 여러개의 결정 트리를 무작위로 만들어서 예측을 하는 앙상블 모델
다수결로 최종 결과를 산출 - 1. 안정성 2. 여러개의 모델의 평가를 거치기 때문에 새로 모든 데이터에 대한 판단

파라미터
Mean Sample Split

베이스라인 모델을 넘겨준 다음 다시한번 튜닝

하이퍼파라미터 튜닝에 최적화된 모델 : optuna
optuna라이브러리를 얹어서


#### Ensemble(앙상블)
여러 개의 모델을 결합해서 더 강력한 모델을 만드는 기법!

| Bagging(Bootstrap Aggregating) | boosting |

Bagging(Bootstrap Aggregating)
여러 모델을 독립적으로 학습한 후 평균(회귀) 또는 다수결(분류)로 최종 예측을 내는 방법
여러 개의 약한 모델을 조합해서 강한 모델을 만드는 것이다.

1. 데이터를 랜덤하게 샘플링 (부트스트랩핑)
2. 독립적인 모델을 학습 (약한 학습기)

모든 모델이 독립적으로 학습되므로 빠르다.

대표적 알고리즘 ➡️ 랜덤 포레스트




***
#### 우리FISA 38일차 KPT